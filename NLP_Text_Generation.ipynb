{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuWLZXuIu65P"
   },
   "source": [
    "# Generating Shakespearean Text with Character Based RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1c5v5Tcu65S"
   },
   "source": [
    "Problem Statement:\n",
    "Given a character or sequence of characters, we want to predict the next character at each time step. Model is trained to follow a language similar to the works of Shakespeare. The tinyshakespear dataset is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEFU8KZ2u65T"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nW3mL0vu65i"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "wh4SN7p3u65j",
    "outputId": "bff63f1e-f4b7-4fe9-d633-32d59dfc2874"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-052c6f1c-73e8-491c-85c2-2dfddb56aa1b\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-052c6f1c-73e8-491c-85c2-2dfddb56aa1b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving shakespeare.txt to shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "#Dataset source:https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare \n",
    "#Did not use the original stanford dataset (https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt); too long- increases processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "zhltKBH8u65o",
    "outputId": "bda2bf8c-7398-4ec7-aaa4-a236df9f4415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "#check if decoding is needed: text may need to be decoded as utf-8\n",
    "text = open('shakespeare.txt', 'r').read() \n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vhh1aNhSu651",
    "outputId": "e7dedc13-8316-44f6-9ab8-3180b4fa090d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "#Find Vocabulary (set of characters)\n",
    "vocabulary = sorted(set(text))\n",
    "print('No. of unique characters: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqSez6Ipu657"
   },
   "source": [
    " ## Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T43LgDlpu66A"
   },
   "outputs": [],
   "source": [
    "#character to index mapping\n",
    "char2index = {c:i for i,c in enumerate(vocabulary)}\n",
    "int_text = np.array([char2index[i] for i in text])\n",
    "\n",
    "#Index to character mapping\n",
    "index2char = np.array(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xdUvYk8au66Z",
    "outputId": "71977e03-b494-4775-9305-0ff0716e992e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to Index: \n",
      "\n",
      "  '\\n':   0\n",
      "  ' ' :   1\n",
      "  '!' :   2\n",
      "  '$' :   3\n",
      "  '&' :   4\n",
      "  \"'\" :   5\n",
      "  ',' :   6\n",
      "  '-' :   7\n",
      "  '.' :   8\n",
      "  '3' :   9\n",
      "  ':' :  10\n",
      "  ';' :  11\n",
      "  '?' :  12\n",
      "  'A' :  13\n",
      "  'B' :  14\n",
      "  'C' :  15\n",
      "  'D' :  16\n",
      "  'E' :  17\n",
      "  'F' :  18\n",
      "  'G' :  19\n",
      "  'H' :  20\n",
      "  'I' :  21\n",
      "  'J' :  22\n",
      "  'K' :  23\n",
      "  'L' :  24\n",
      "  'M' :  25\n",
      "  'N' :  26\n",
      "  'O' :  27\n",
      "  'P' :  28\n",
      "  'Q' :  29\n",
      "  'R' :  30\n",
      "  'S' :  31\n",
      "  'T' :  32\n",
      "  'U' :  33\n",
      "  'V' :  34\n",
      "  'W' :  35\n",
      "  'X' :  36\n",
      "  'Y' :  37\n",
      "  'Z' :  38\n",
      "  'a' :  39\n",
      "  'b' :  40\n",
      "  'c' :  41\n",
      "  'd' :  42\n",
      "  'e' :  43\n",
      "  'f' :  44\n",
      "  'g' :  45\n",
      "  'h' :  46\n",
      "  'i' :  47\n",
      "  'j' :  48\n",
      "  'k' :  49\n",
      "  'l' :  50\n",
      "  'm' :  51\n",
      "  'n' :  52\n",
      "  'o' :  53\n",
      "  'p' :  54\n",
      "  'q' :  55\n",
      "  'r' :  56\n",
      "  's' :  57\n",
      "  't' :  58\n",
      "  'u' :  59\n",
      "  'v' :  60\n",
      "  'w' :  61\n",
      "  'x' :  62\n",
      "  'y' :  63\n",
      "  'z' :  64\n",
      "\n",
      "Input text to Integer: \n",
      "\n",
      "'First Citizen:\\nBefor' mapped to [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print(\"Character to Index: \\n\")\n",
    "for char,_ in zip(char2index, range(65)):\n",
    "    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n",
    "\n",
    "print(\"\\nInput text to Integer: \\n\")\n",
    "print('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgStBpG7u66q"
   },
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZD81sKbu66v"
   },
   "outputs": [],
   "source": [
    "seq_length= 150 #max number of characters that can be fed as a single input\n",
    "examples_per_epoch = len(text)\n",
    "\n",
    "#converts text (vector) into character index stream\n",
    "#Reference: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(int_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7jMCKJpu66z"
   },
   "outputs": [],
   "source": [
    "#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "vclYXysDu662",
    "outputId": "39e2c742-1c58-4461-f86e-b55980ce4725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Stream: \n",
      "\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "\n",
      "Sequence: \n",
      "\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n",
      "\"l:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
      "\"ill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good \"\n",
      "'citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but '\n",
      "'the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the '\n",
      "'object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we '\n",
      "'become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caiu'\n",
      "\"s Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst \"\n",
      "'Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not m'\n",
      "'aliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was'\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print(\"Character Stream: \\n\")\n",
    "for i in char_dataset.take(10):\n",
    "  print(index2char[i.numpy()])  \n",
    "\n",
    "print(\"\\nSequence: \\n\")\n",
    "for i in sequences.take(10):\n",
    "  print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ncgUIEcu67D"
   },
   "source": [
    "Target value: for each sequence of characters, we return that sequence, shifted one position to the right, along with the new character that is predicted to follow the sequence.\n",
    "\n",
    "To create training examples of (input, target) pairs, we take the given sequence. The input is sequence with last word removed. Target is sequence with first word removed. \n",
    "Example: \n",
    "sequence: abc d ef\n",
    "input: abc d e\n",
    "target: bc d ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNS5cL6pu67D"
   },
   "outputs": [],
   "source": [
    "def create_input_target_pair(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(create_input_target_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "LFYIg0fdu67Q",
    "outputId": "4744307d-3ed9-46a7-d2bd-34da72e2f3f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nA'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAl'\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(index2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9kRvh8uru67X",
    "outputId": "8f2eda4f-5fdb-4eac-e5e5-69f7d2b5dfb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer used to shuffle the dataset \n",
    "# Reference: https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsM3ZPlbu67b"
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMa6-2u6u67c"
   },
   "source": [
    "## Building the Model- GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmNE0oiYu67d"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 256\n",
    "rnn_units= 1024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pn6X5evku67m"
   },
   "source": [
    "3 Layers used:\n",
    "1. Input Layer: Maps character to 256 dimension vector\n",
    "2. GRU Layer: RNN of size 1024\n",
    "3. Dense Layer: Output with same size as vocabulary\n",
    "\n",
    "Since it is a character level RNN, we can use keras.Sequential model (All layers have single input and single output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "we7fYqkIu67n"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units, \n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "# Reference for GRU: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "# Reference for theory: https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9KvHsCwu67q"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZR7xWtQdu67y"
   },
   "outputs": [],
   "source": [
    "#Testing: shape\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_prediction = model(input_example_batch)\n",
    "    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n",
    "    #print(example_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8zsYRE9u675"
   },
   "outputs": [],
   "source": [
    "#model.summary() \n",
    "#check shapes if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlLJC9dzu679"
   },
   "outputs": [],
   "source": [
    "#Sampling the distribution- gives predicted next character at every timestamp (Untrained)\n",
    "sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xebhwspu68D"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-PMMMU8pu68E",
    "outputId": "ca686faa-03bc-4e54-b2ea-88ae8692e901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 150, 65)\n",
      "Loss:       4.174315\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "#Loss Function reference: https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/\n",
    "\n",
    "example_loss  = loss(target_example_batch, example_prediction)\n",
    "print(\"Prediction shape: \", example_prediction.shape)\n",
    "print(\"Loss:      \", example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eb29yNI3u68K"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K62FfwWsu68N"
   },
   "outputs": [],
   "source": [
    "#Save model after every epoch (training was time consuming). \n",
    "#Reference: https://medium.com/@italojs/saving-your-weights-for-each-epoch-keras-callbacks-b494d9648202\n",
    "\n",
    "dir_checkpoints= './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(dir_checkpoints, \"checkpt_{epoch}\") #name\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHfKjiWru68S"
   },
   "outputs": [],
   "source": [
    "EPOCHS=50 #increase number of epochs for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-xQ1DDUAu68W",
    "outputId": "85ed1d85-1eae-402d-c614-6c390f23ad23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 2.8494\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 2.1127\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.8455\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.6633\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.5413\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.4611\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.4029\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.3577\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.3189\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.2855\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.2545\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.2251\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.1962\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.1678\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.1379\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.1079\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.0771\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.0453\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 1.0113\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.9766\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 7s 61ms/step - loss: 0.9439\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 7s 61ms/step - loss: 0.9086\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.8720\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.8386\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.8047\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.7743\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.7432\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.7165\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.6918\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.6692\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.6496\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.6308\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.6156\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5997\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5872\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5769\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5645\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5576\n",
      "Epoch 39/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5478\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5407\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5360\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5296\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5220\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5185\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5109\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5079\n",
      "Epoch 47/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.5037\n",
      "Epoch 48/50\n",
      "115/115 [==============================] - 7s 59ms/step - loss: 0.5010\n",
      "Epoch 49/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.4980\n",
      "Epoch 50/50\n",
      "115/115 [==============================] - 7s 60ms/step - loss: 0.4951\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zgzK6rNgyGVf",
    "outputId": "0bec79b4-c202-4b92-c315-208b1df15b6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/checkpt_50'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(dir_checkpoints) #check last checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e01iHtPxu68e"
   },
   "source": [
    "## Prediction- GRU\n",
    "(Using batch size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "Yb7gQW8pykhY",
    "outputId": "53d7e0fc-e058-4308-dbb2-558505f8dfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(dir_checkpoints))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PeeRK1Ru68k"
   },
   "outputs": [],
   "source": [
    "#Generate text from model\n",
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000 #Number of characters to be generated\n",
    "\n",
    "  input_eval = [char2index[s] for s in start_string] #vectorising input\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.5\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(index2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iugYk5PUu68o"
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "#print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6rTnZl6u68s"
   },
   "outputs": [],
   "source": [
    "#test = input(\"Enter your starting string: \")\n",
    "#print(generate_text(model, start_string=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "_t5H_5iobn_C",
    "outputId": "c74ccc5b-7fcf-4012-f980-6e467af51a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your starting string: The \n",
      "The outward war with him!\n",
      "\n",
      "Third Servingman:\n",
      "Why, then the widow of the senate, that\n",
      "in some store of law to him and Margaret:\n",
      "But if you have a helple suitors holds from her,\n",
      "Betroth'd and labourinader, have frown crafts\n",
      "To Sly man and her a secret country's crown,\n",
      "Which I do last pronounce, it is not mortal to\n",
      "him.\n",
      "\n",
      "Clown:\n",
      "Is there a man whose bolder dried blood that lies\n",
      "From many a gentleman that bear the war;\n",
      "Bear her most sweet work, you are the measure\n",
      "As you intended to thy crown; come, take my mind,\n",
      "And in my houses of the news then the world\n",
      "Were foundage in me.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "That's my good son: he is coming hither.\n",
      "\n",
      "PETRUCHIO:\n",
      "Then thus: I have fed upon me: I have reason;\n",
      "And there I am committed.\n",
      "\n",
      "Provost:\n",
      "I cannot bloody tride or no rage doth grow.\n",
      "\n",
      "CATESBY:\n",
      "Fie, and but I will marry you. Go you see, I think,\n",
      "Thou mayst thine ears and restluction of a guilty house!\n",
      "Mean me in prothecount that you love my brother.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "First Senator:\n",
      "Farewell, sweet lords; the Duke of Mi\n"
     ]
    }
   ],
   "source": [
    "#Predication with User Input\n",
    "gru_test = input(\"Enter your starting string: \")\n",
    "print(generate_text(model, start_string=gru_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_pXX2i6u683"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okWohtc5u684"
   },
   "source": [
    "## Building the Model- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTEBkqNmu685"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 256\n",
    "rnn_units= 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdWkoBl5u688"
   },
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units, \n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJ8bkg1Nu69A"
   },
   "outputs": [],
   "source": [
    "lstm_model = build_model_lstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtkb-JL8u69I"
   },
   "outputs": [],
   "source": [
    "#Testing: shape\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_prediction = lstm_model(input_example_batch)\n",
    "    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n",
    "    #print(example_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-I2XpKzgu69N"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SGjASvVHu69P",
    "outputId": "df1c43ed-af66-4d49-bbb6-052f4b5535ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 150, 65)\n",
      "Loss:       4.1740937\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "#Loss Function reference: https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/\n",
    "\n",
    "example_loss  = loss(target_example_batch, example_prediction)\n",
    "print(\"Prediction shape: \", example_prediction.shape)\n",
    "print(\"Loss:      \", example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKu-zH6fu69S"
   },
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvGv1oxJu69a"
   },
   "outputs": [],
   "source": [
    "lstm_dir_checkpoints= './training_checkpoints_LSTM'\n",
    "checkpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuQtRID5u69b"
   },
   "outputs": [],
   "source": [
    "EPOCHS=60 #increase number of epochs for better results (lesser loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "34lmDTKNu69i",
    "outputId": "87dd2749-cd4e-4e54-d887-632a0088b3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "115/115 [==============================] - 9s 76ms/step - loss: 2.7611\n",
      "Epoch 2/60\n",
      "115/115 [==============================] - 9s 76ms/step - loss: 2.0264\n",
      "Epoch 3/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.7567\n",
      "Epoch 4/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.5940\n",
      "Epoch 5/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.4897\n",
      "Epoch 6/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 1.4189\n",
      "Epoch 7/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 1.3653\n",
      "Epoch 8/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.3227\n",
      "Epoch 9/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.2854\n",
      "Epoch 10/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 1.2501\n",
      "Epoch 11/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.2172\n",
      "Epoch 12/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.1848\n",
      "Epoch 13/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.1521\n",
      "Epoch 14/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 1.1191\n",
      "Epoch 15/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 1.0847\n",
      "Epoch 16/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 1.0481\n",
      "Epoch 17/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 1.0119\n",
      "Epoch 18/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.9733\n",
      "Epoch 19/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.9331\n",
      "Epoch 20/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.8929\n",
      "Epoch 21/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.8539\n",
      "Epoch 22/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.8143\n",
      "Epoch 23/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.7763\n",
      "Epoch 24/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.7382\n",
      "Epoch 25/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.7050\n",
      "Epoch 26/60\n",
      "115/115 [==============================] - 9s 76ms/step - loss: 0.6716\n",
      "Epoch 27/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.6405\n",
      "Epoch 28/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.6138\n",
      "Epoch 29/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.5877\n",
      "Epoch 30/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.5615\n",
      "Epoch 31/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.5387\n",
      "Epoch 32/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.5197\n",
      "Epoch 33/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.5012\n",
      "Epoch 34/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.4847\n",
      "Epoch 35/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.4705\n",
      "Epoch 36/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.4557\n",
      "Epoch 37/60\n",
      "115/115 [==============================] - 9s 76ms/step - loss: 0.4420\n",
      "Epoch 38/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.4321\n",
      "Epoch 39/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.4199\n",
      "Epoch 40/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.4108\n",
      "Epoch 41/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.4013\n",
      "Epoch 42/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.3921\n",
      "Epoch 43/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3869\n",
      "Epoch 44/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.3779\n",
      "Epoch 45/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3723\n",
      "Epoch 46/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3674\n",
      "Epoch 47/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3633\n",
      "Epoch 48/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3577\n",
      "Epoch 49/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3512\n",
      "Epoch 50/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3470\n",
      "Epoch 51/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.3437\n",
      "Epoch 52/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3407\n",
      "Epoch 53/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.3379\n",
      "Epoch 54/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3351\n",
      "Epoch 55/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3324\n",
      "Epoch 56/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3303\n",
      "Epoch 57/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.3257\n",
      "Epoch 58/60\n",
      "115/115 [==============================] - 9s 77ms/step - loss: 0.3239\n",
      "Epoch 59/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3234\n",
      "Epoch 60/60\n",
      "115/115 [==============================] - 9s 78ms/step - loss: 0.3236\n"
     ]
    }
   ],
   "source": [
    "history = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HE7iqyZcu69o",
    "outputId": "d7f70da8-10ae-47e6-c23e-25fd4eb2a3c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints_LSTM/checkpt_60'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(lstm_dir_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x_Tb3OWu69s"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "krpYugapu69s",
    "outputId": "5fdd291c-f548-468c-b54f-a3d580e5418f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "lstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\n",
    "lstm_model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Ii1Ib-Cu69v"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000 #Number of characters to be generated\n",
    "\n",
    "  input_eval = [char2index[s] for s in start_string] #vectorising input\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.5\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(index2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n03cYhgYu69x"
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "#print(generate_text(lstm_model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1kdtfjvu693"
   },
   "outputs": [],
   "source": [
    "#test = input(\"Enter your starting string: \")\n",
    "#print(generate_text(lstm_model, start_string=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "8gx1UlaC1CAY",
    "outputId": "42ac6f76-83c1-46b8-f436-a420db45b381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your starting string: The\n",
      "The are thy hearts to me as a face o' the year of the\n",
      "shepherd; it may be so highness and his dishonour dies,\n",
      "Or my shamed life in quarrel of his. I awain\n",
      "That would be to be earthe nothing but one of your not venturous.\n",
      "\n",
      "ANGELO:\n",
      "Believe me, on mine honour,\n",
      "I'll undertake to past her friends: you have\n",
      "not be so loud.\n",
      "\n",
      "BENVOLIO:\n",
      "At this same authority: if you refuse\n",
      "Why, then both proud and not answer to his majesty.\n",
      "Farewell: she shall not seem to be thought upon\n",
      "The people are inclined to his character, much in praters.\n",
      "\n",
      "AUTOLYCUS:\n",
      "\n",
      "Clown:\n",
      "I will to your court, she had as lief\n",
      "Than ever the vantage of his looks I give the lord.\n",
      "\n",
      "BRAKENBURY:\n",
      "What says he?\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Nay, mark mad you mean no herself approaches.\n",
      "\n",
      "MENENIUS:\n",
      "An e'er speak with you.\n",
      "\n",
      "PROSPERO:\n",
      "This better enter, to make a lusty women\n",
      "Ere he the blood of MEnour prodigalities,\n",
      "And say 'tis noble and my promise\n",
      "That long shut thee, to the whole body. The king shall do it.\n",
      "The Eath is brother'd in the house of York,\n",
      "They do\n"
     ]
    }
   ],
   "source": [
    "#Prediction with User Input\n",
    "lstm_test = input(\"Enter your starting string: \")\n",
    "print(generate_text(lstm_model, start_string=lstm_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1um4SUacAEV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Text_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
